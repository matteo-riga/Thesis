{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a7e06d7-6aa1-4356-a52c-41366e8e6d0c",
   "metadata": {},
   "source": [
    "# Deep Learning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d12fb5-6436-4e8e-bd59-3a039a6d9b48",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Imports and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60881ed9-db96-4fbf-b155-806b0b1bd51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import Reader\n",
    "import LogKeysManager\n",
    "import ParamsExtractor\n",
    "import DataPreprocessor\n",
    "import ReduceDim\n",
    "import ClusterData\n",
    "import OneClass\n",
    "import DetectAnomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ab24d3-0d48-4a1a-928b-8b0f430043a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbc51a6-055e-4384-b0ae-72f6cf8c3852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', size=16)\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7382b723-8a50-4d66-bb23-89a187c4d7ae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### fix randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f3497d-c3cd-4e9e-a81e-d03f185ef284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix randomness and hide warnings\n",
    "seed = 42\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "\n",
    "import logging\n",
    "\n",
    "import random\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731f8c18-7575-4ce1-82a0-7297ab379afd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### file reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1054c5a7-fc83-40f2-b65a-bceae52e3cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Params Extractor\n",
    "#log_types = ['cron', 'user']\n",
    "log_types = ['cron', 'laurel', 'maillog', 'messages', 'secure', 'user']\n",
    "file_paths = ['../../../temp_logs/login01.' + logtype + '.log' for logtype in log_types]\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    r = Reader.Reader(file_path)\n",
    "    df = r.read_file()\n",
    "    df_list.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59217d59-4195-43d0-8104-e18d38a2e343",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_after = []\n",
    "\n",
    "for i, df in enumerate(df_list):\n",
    "    p = ParamsExtractor.ParamsExtractor(df)\n",
    "    df = p.convert_params(df)\n",
    "    new_df = p.get_params()\n",
    "    df_after.append(new_df)\n",
    "\n",
    "for i in range(len(df_list)):\n",
    "    #print('=================')\n",
    "    #print(df_list[i].head())\n",
    "    #print(df_list[i].columns)\n",
    "    #print(df_list[i].iloc[0])\n",
    "    #print('********')\n",
    "    #print(df_after[i].head())\n",
    "    #print(df_after[i].columns)\n",
    "    #print(df_after[i].iloc[0])\n",
    "    \n",
    "    # Concatenate df and df_after\n",
    "    df_list[i] = pd.concat([df_list[i], df_after[i]], axis=1)\n",
    "    #print(df_list[i].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5e8275-9e93-4880-b882-1f3deb93e1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprcessor\n",
    "\n",
    "for i, df in enumerate(df_list):\n",
    "    d = DataPreprocessor.DataPreprocessor(df)\n",
    "    enc = d.drop_and_one_hot_encode()\n",
    "    df_list[i] = enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daca02bd-1de0-4b5b-b7e5-79eedfa4c92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a648b3-cefe-4952-8c69-9e9652a2c742",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5768684f-e054-4666-afb0-733f903bd620",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data\n",
    "\n",
    "# Split data into train_val and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, random_state=seed, test_size=.25, stratify=np.argmax(y,axis=1))\n",
    "\n",
    "# Further split train_val into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=seed, test_size=len(X_test), stratify=np.argmax(y_train_val,axis=1))\n",
    "\n",
    "# Print shapes of the datasets\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d9abdd-5251-4855-983a-e7f875ca353d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shape, output shape, batch size, and number of epochs\n",
    "input_shape = X_train.shape[1:]\n",
    "output_shape_not_expanded = y_train.shape[1:]\n",
    "output_shape = np.expand_dims(output_shape_not_expanded, axis=-1)\n",
    "batch_size = 32\n",
    "epochs = 1000\n",
    "\n",
    "# Print input shape, batch size, and number of epochs\n",
    "print(f\"Input Shape: {input_shape}, Output Shape: {output_shape}, Batch Size: {batch_size}, Epochs: {epochs}\")\n",
    "\n",
    "learning_rate = 1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4163f4c2-8fa0-4451-bbbc-c875703483ff",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a258117-cc52-47d8-a308-816fee799e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_LeNet(input_shape=input_shape, output_shape=output_shape, seed=seed):\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    # Build the neural network layer by layer\n",
    "    input_layer = tfkl.Input(shape=input_shape, name='Input')\n",
    "\n",
    "    conv1 = tfkl.Conv2D(\n",
    "        filters=6,\n",
    "        kernel_size=(5,5),\n",
    "        padding='same',\n",
    "        activation='tanh',\n",
    "        name='conv1'\n",
    "    )(input_layer)\n",
    "\n",
    "    pool1 = tfkl.MaxPooling2D(\n",
    "        pool_size=(2,2),\n",
    "        name='mp1'\n",
    "    )(conv1)\n",
    "\n",
    "    conv2 = tfkl.Conv2D(\n",
    "        filters=16,\n",
    "        kernel_size=(5,5),\n",
    "        padding='valid',\n",
    "        activation='tanh',\n",
    "        name='conv2'\n",
    "    )(pool1)\n",
    "\n",
    "    pool2 = tfkl.MaxPooling2D(\n",
    "        pool_size =(2,2),\n",
    "        name='mp2'\n",
    "    )(conv2)\n",
    "\n",
    "    flattening_layer=tfkl.Flatten(\n",
    "        name='flatten'\n",
    "    )(pool2)\n",
    "\n",
    "    classifier_layer=tfkl.Dense(\n",
    "        units=120,\n",
    "        activation='tanh',\n",
    "        name='dense1'\n",
    "    )(flattening_layer)\n",
    "\n",
    "    classifier_layer = tfkl.Dense(\n",
    "        units=84,\n",
    "        activation='tanh',\n",
    "        name='dense2'\n",
    "    )(classifier_layer)\n",
    "\n",
    "    output_layer = tfkl.Dense(\n",
    "        units=output_shape,\n",
    "        activation='softmax',\n",
    "        name='Output'\n",
    "    )(classifier_layer)\n",
    "\n",
    "    # Connect input and output through the Model class\n",
    "    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='LeNet')\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics=['accuracy'])\n",
    "    # model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.AdamW(), metrics=['accuracy']) # AdamW applies the L2-norm. Extra stuff might be rewarded in the competition.\n",
    "\n",
    "    # Return the model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18a6a90-994b-4a3d-b812-6e1f9e6d75ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_LeNet(input_shape, output_shape)\n",
    "model.summary()\n",
    "tfk.utils.plot_model(model, expand_nested=True, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c88a2d9-5698-4d98-a441-31b0eb2913d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    x = X_train,\n",
    "    y = y_train,\n",
    "    batch_size = batch_size,\n",
    "    epochs = epochs,\n",
    "    validation_data = (X_val, y_val)\n",
    ").history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1f6b36-339a-46b5-9b4c-af96bf8fb6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(history['loss'], alpha=.3, color='#ff7f0e', linestyle='--')\n",
    "plt.plot(history['val_loss'], label='LeNet', alpha=.8, color='#ff7f0e')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Categorical Crossentropy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(history['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')\n",
    "plt.plot(history['val_accuracy'], label='LeNet', alpha=.8, color='#ff7f0e')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Accuracy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a084f6a-ff18-492f-8352-5314635815f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
