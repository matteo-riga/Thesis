{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41d29922-96d3-47d7-a3b6-6b6ed2850942",
   "metadata": {},
   "source": [
    "# Normal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "851dc3c6-168e-462b-a335-3bd5a7a9b5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 12:12:55.369119: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-05 12:12:55.418876: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-05 12:12:56.192636: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "sys.path.append('../')\n",
    "sys.path.append('../src/')\n",
    "sys.path.append('../spell/')\n",
    "\n",
    "import Reader\n",
    "import ParamsExtractor3\n",
    "import DataPreprocessor\n",
    "import EncodeCommand\n",
    "import DeepLearningAnomalyDetection2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "096a8bb7-cdf0-41d3-80f6-5779f5342d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/new_laurel_conf/audit.log']\n"
     ]
    }
   ],
   "source": [
    "log_types = ['laurel']\n",
    "\n",
    "# Generate the list of file paths\n",
    "file_paths = [f'../data/new_laurel_conf/audit.log' for logtype in log_types]\n",
    "\n",
    "# Filter the list to include only existing files\n",
    "fps = [path for path in file_paths if os.path.exists(path)]\n",
    "print(fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "093b656c-83cb-4fad-b9dd-ad402d6705df",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = fps[0]\n",
    "\n",
    "data_list = []\n",
    "lines_limit = 100000 # limit reading lines, done for memory constraints\n",
    "i = 0\n",
    "\n",
    "with open(fp, 'r') as file:\n",
    "    for line in file:\n",
    "        i+=1\n",
    "        if i > lines_limit:\n",
    "            break\n",
    "            # Find the index of the first '{'\n",
    "        index = line.find('{')\n",
    "        if index != -1:\n",
    "            json_data = line[index:]\n",
    "            data = json.loads(json_data)\n",
    "            data_list.append(data)\n",
    "                \n",
    "df = pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "886409fb-c2f0-450b-8d37-d04f78b05eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(fp):\n",
    "    data_list = []\n",
    "    lines_limit = 1000 # limit reading lines, done for memory constraints\n",
    "    i = 0\n",
    "    \n",
    "    with open(fp, 'r') as file:\n",
    "        for line in file:\n",
    "            i+=1\n",
    "            if i > lines_limit:\n",
    "                break\n",
    "                # Find the index of the first '{'\n",
    "            index = line.find('{')\n",
    "            if index != -1:\n",
    "                json_data = line[index:]\n",
    "                data = json.loads(json_data)\n",
    "                data_list.append(data)\n",
    "                    \n",
    "    df = pd.DataFrame(data_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "188d1bcd-abd8-43a4-a1cd-5588a263c26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df, col):\n",
    "    if col not in df.columns:\n",
    "        print(f\"Error, column {col} not in df\")\n",
    "        return None\n",
    "    df_expanded = pd.json_normalize(df[col])\n",
    "    #print(df_expanded.head())\n",
    "    df = df.drop(col, axis=1)\n",
    "    df = df.join(df_expanded, rsuffix=f'_{col}')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bca1163-8d1e-4ed5-aa5b-9eafb8010ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Preprocessing Function\n",
    "def preprocess_laurel(df):\n",
    "    \n",
    "    df = normalize(df, 'CWD')\n",
    "    df = normalize(df, 'PATH')\n",
    "    df = normalize(df, 'SYSCALL')\n",
    "    df = normalize(df, 'PROCTITLE')\n",
    "    df = normalize(df, 'EXECVE')\n",
    "    df = normalize(df, 0)\n",
    "    df = normalize(df, 1)\n",
    "    print(df.head())\n",
    "\n",
    "    columns_to_keep = ['cwd', 'exit', 'items', 'ppid', 'pid', 'comm', 'timedelta', 'pid_timedelta', 'ppid_timedelta', 'id_anomalies', 'num_id_anomalies', 'ARGV_PROCTITLE_str']\n",
    "    columns_to_drop = ['ID', 'PID.EVENT_ID', 'PPID.EVENT_ID', 'unix_time', 'ARGV_PROCTITLE', 'time', 'pid_time', 'ppid_time']\n",
    "    \n",
    "    try:\n",
    "        df['ARGV_PROCTITLE_str'] = df['ARGV_PROCTITLE'].apply(lambda x: ' '.join(x))\n",
    "    except:\n",
    "        columns_to_keep.remove('ARGV_PROCTITLE_str')\n",
    "        pass\n",
    "    \n",
    "    df['unix_time'] = pd.to_numeric(df['ID'].str.split(':').str[0])\n",
    "    df['timedelta'] = df['unix_time'].diff()\n",
    "    df['time'] = pd.to_datetime(df['unix_time'], unit='s')\n",
    "\n",
    "    try:\n",
    "        df['ppid_time'] = pd.to_datetime(pd.to_numeric(df['PPID.EVENT_ID'].str.split(':').str[0]), unit='s')\n",
    "        df['ppid_timedelta'] = df['ppid_time'].diff()\n",
    "        columns_to_keep.remove('ppid_timedelta')\n",
    "        columns_to_drop.remove('ppid_time')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    df['pid_time'] = pd.to_datetime(pd.to_numeric(df['PID.EVENT_ID'].str.split(':').str[0]), unit='s')\n",
    "    df['pid_timedelta'] = df['pid_time'].diff()\n",
    "    \n",
    "    df = df.drop(columns_to_drop, axis=1)\n",
    "\n",
    "    columns = ['auid', 'uid', 'gid', 'euid', 'suid', 'fsuid', 'egid', 'sgid', 'fsgid']\n",
    "    \n",
    "    def check_row_id_num(row):\n",
    "        return pd.Series({col: row[col] == 0 for col in columns})\n",
    "    \n",
    "    checks = df.apply(check_row_id_num, axis=1)\n",
    "    df['num_id_anomalies'] = checks.sum(axis=1)\n",
    "    df = df.drop(columns, axis=1)\n",
    "\n",
    "    columns = ['AUID', 'UID', 'GID', 'EUID', 'SUID', 'FSUID', 'EGID', 'SGID', 'FSGID']\n",
    "    \n",
    "    # Use apply to efficiently check values for each row\n",
    "    def check_id_row(row):\n",
    "        try:\n",
    "            return pd.Series({col: int(not(row[col] in row['UID_GROUPS'])) for col in columns})\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    checks = df.apply(check_id_row, axis=1)\n",
    "    \n",
    "    df['id_anomalies'] = checks.sum(axis=1)\n",
    "    df = df.drop(columns, axis=1)\n",
    "    df = df.drop('UID_GROUPS', axis=1)\n",
    "\n",
    "    # filter final columns\n",
    "    df = df[columns_to_keep]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c20efdc1-1ee9-4a17-ae1d-c0d24f223d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(df, save_fp):\n",
    "    # Save this df to file\n",
    "    #save_fp = '../data/laurel_anomalous_new/save1.csv'\n",
    "    df.to_csv(save_fp, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d611d2a-073b-4cca-8f99-eb08266f9057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   ID SERVICE_STOP BPRM_FCAPS SERVICE_START SYSTEM_RUNLEVEL  \\\n",
      "0  1721464625.772:929          NaN        NaN           NaN             NaN   \n",
      "1  1721464625.816:930          NaN        NaN           NaN             NaN   \n",
      "2  1721464625.641:933          NaN        NaN           NaN             NaN   \n",
      "3  1721464625.512:934          NaN        NaN           NaN             NaN   \n",
      "4  1721464625.636:935          NaN        NaN           NaN             NaN   \n",
      "\n",
      "   BPF USER_AUTH USER_ACCT CRED_ACQ LOGIN  ... rdev_1  \\\n",
      "0  NaN       NaN       NaN      NaN   NaN  ...  00:00   \n",
      "1  NaN       NaN       NaN      NaN   NaN  ...  00:00   \n",
      "2  NaN       NaN       NaN      NaN   NaN  ...  00:00   \n",
      "3  NaN       NaN       NaN      NaN   NaN  ...  00:00   \n",
      "4  NaN       NaN       NaN      NaN   NaN  ...  00:00   \n",
      "\n",
      "                          obj_1 nametype_1 cap_fp_1 cap_fi_1 cap_fe_1  \\\n",
      "0  system_u:object_r:ld_so_t:s0     NORMAL      0x0      0x0      0.0   \n",
      "1  system_u:object_r:ld_so_t:s0     NORMAL      0x0      0x0      0.0   \n",
      "2  system_u:object_r:ld_so_t:s0     NORMAL      0x0      0x0      0.0   \n",
      "3  system_u:object_r:ld_so_t:s0     NORMAL      0x0      0x0      0.0   \n",
      "4  system_u:object_r:ld_so_t:s0     NORMAL      0x0      0x0      0.0   \n",
      "\n",
      "  cap_fver_1 cap_frootid_1 OUID_1  OGID_1  \n",
      "0        0x0             0   root    root  \n",
      "1        0x0             0   root    root  \n",
      "2        0x0             0   root    root  \n",
      "3        0x0             0   root    root  \n",
      "4        0x0             0   root    root  \n",
      "\n",
      "[5 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "an_df_list = []\n",
    "\n",
    "for fp in fps:\n",
    "    df = read(fp)\n",
    "    df = preprocess_laurel(df)\n",
    "    save(df, fp + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a793c4c6-027e-4fad-bc9b-fc7c98d80a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_dataset_fp = fp + '.csv'\n",
    "new_path = '../cleaned_data/normal.csv'\n",
    "save(df, new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0119c1c9-f826-4ca8-92c3-7875b4098486",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
